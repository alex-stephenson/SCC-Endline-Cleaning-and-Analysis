cleaned_data$CARI_ECMEN <- factor(cleaned_data$CARI_ECMEN,
levels = c(1, 2, 3, 4),
labels = CARI_ECMEN_labels)
sampling_frame <- read_csv("inputs/weights.csv")
cleaned_data_weighted <- cleaned_data %>%
add_weights(sampling_frame,
strata_column_dataset = "district",
strata_column_sample = "strata.names",
population_column = "population")
# writing the raw and cleaned data
data<-list(questions =questions,choices=choices,raw_data=raw_data,cleaned_data=cleaned_data_weighted)
writexl::write_xlsx(data,paste("output/clean_and_raw.xlsx"))
###########################################################
########################## Setup ##########################
###########################################################
rm(list = ls())
library(tidyverse)
library(cleaningtools)
library(analysistools)
library(presentresults)
library(readxl)
# write the aggregation file with a timestamp to more easily keep track of different versions
date_time_now <- format(Sys.time(), "%a_%b_%d_%Y_%H%M%S")
# load datasets for processing
file_path <- "output/clean_and_raw.xlsx"
main_data <- read_excel(file_path, 'cleaned_data')
raw_data <- read_excel("inputs/as_raw_data.xlsx") %>%
select(-savings)
## tool
kobo_tool_name <- "inputs/IRF_ENDLINE_TOOL_FEB2025.xlsx"
kobo_survey <- read_excel(kobo_tool_name, sheet = "survey") %>%
mutate(type = str_squish(type)) %>%
mutate(`label::English` = ifelse(
`label::English` == "If yes, which goods and commodity prices have increased?" & name == "community_price_goods", "If yes, which community goods and commodity prices have increased?", `label::English`))
kobo_choice <- read_excel(kobo_tool_name, sheet = "choices") %>%
distinct(list_name, name, .keep_all = T) %>%
filter(list_name != "settlement" | list_name == "settlement" & name %in% main_data$settlement) %>%
filter(name != "Bulsho IDP")
## load in the LOA
loa <- readxl::read_excel("inputs/scc_endline_loa.xlsx")
deletion <- read_excel("inputs/as_deletion_log.xlsx")
combined_clogs <- read_excel("combined clogs/corrected_combined cleaning clogs.xlsx") %>%
filter(! uuid %in% deletion$uuid)
############################### create HH survey design and analysis #################################################
#main_data_cols <- main_data %>%
#  select(any_of(contains(loa_questions)), weights)
SCC_endline_Survey_Design <- main_data %>%
srvyr::as_survey_design(., strata = "district", weights = weights)
my_analysis <- create_analysis(SCC_endline_Survey_Design, sm_separator = "/", loa = loa)
results_table <- my_analysis$results_table
## remove some issues highlighted in the review_kobo_labels
review_kobo_labels_results <- review_kobo_labels(kobo_survey,
kobo_choice,
label_column = "label::English",
results_table = results_table)
label_dictionary <- create_label_dictionary(kobo_survey,
kobo_choice,
label_column = "label::English",
results_table = results_table)
results_table_labeled <- add_label_columns_to_results_table(
results_table,
label_dictionary
)
### making of the tables
df_main_analysis_table <- presentresults::create_table_variable_x_group(results_table = results_table_labeled, value_columns = "stat")
# Replace NA values in list columns with NULL
df_main_analysis_table <- df_main_analysis_table %>%
mutate(across(where(is.list), ~ map(.x, ~ ifelse(is.na(.x), list(NULL), .x))))
# # Replace NA values in non-list columns with "NA"
# df_main_analysis_table <- df_main_analysis_table %>%
#   mutate(across(where(~ !is.list(.x)), ~ ifelse(is.na(.x), "NA", .x)))
df_main_analysis_table <- df_main_analysis_table %>%
mutate(across(where(~ !is.list(.x) & is.numeric(.x)), ~ replace(.x, is.na(.x), NA))) %>%
mutate(across(where(~ !is.list(.x) & !is.numeric(.x)), ~ ifelse(is.na(.x), "NA", .x)))
# Step 8: Export the main analysis percentages table -------------------------
presentresults::create_xlsx_variable_x_group(
table_group_x_variable = df_main_analysis_table,
file_path = paste0("output/results_table_long_percent.xlsx"),
value_columns = c("stat","n"),
overwrite = TRUE
)
# Step 9: Create and process the statistics table (counts: n, N, weighted) ----
# Ensure `df_data$results_table` is used correctly for creating a table by group
df_stats_table <- presentresults::create_table_variable_x_group(
results_table = results_table_labeled,
value_columns = c("n")
)
# Handle NA values in df_stats_table
df_stats_table <- df_stats_table %>%
mutate(across(where(is.list), ~ map(.x, ~ ifelse(is.na(.x), list(NULL), .x)))) %>%
mutate(across(where(~ !is.list(.x)), ~ ifelse(is.na(.x), "", .x)))
# Export the processed stats table to Excel
presentresults::create_xlsx_variable_x_group(
table_group_x_variable = df_stats_table,  # Use the processed table
file_path = paste0("output/results_table_long_values.xlsx"),
value_columns = c("n"),
overwrite = TRUE
)
### create wide format now::
df_wide_analysis_table <- presentresults::create_table_group_x_variable(results_table = results_table_labeled, value_columns = "stat")
presentresults::create_xlsx_group_x_variable(
table_group_x_variable = df_wide_analysis_table,
file_path = paste0("output/results_table_wide_percent.xlsx"),
overwrite = TRUE
)
df_wide_analysis_table_num <- presentresults::create_table_group_x_variable(
results_table = results_table_labeled, value_columns = c("n"))
presentresults::create_xlsx_group_x_variable(
table_group_x_variable = df_wide_analysis_table_num,
file_path = paste0("output/results_table_wide_values.xlsx"),
overwrite = TRUE
)
create_variable_tracker <- function(raw_data, clean_data) {
removed_cols <- data.frame(Variable = colnames(raw_kobo_data)) %>%
filter(! x %in% colnames(my_clean_data)) %>%
mutate(Action = "remove",
Rationale = "")
new_cols <- data.frame(Variable = colnames(my_clean_data)) %>%
filter(! x %in% colnames(raw_kobo_data)) %>%
mutate(Action = "add",
Rationale = "")
variable_tracker <- bind_rows(removed_cols, new_cols)
}
new_cols <- data.frame(Variable = colnames(clean_data)) %>%
filter(! x %in% colnames(raw_data)) %>%
mutate(Action = "added",
Rationale = "")
create_variable_tracker <- function(raw_data, clean_data) {
removed_cols <- data.frame(Variable = colnames(raw_data)) %>%
filter(! x %in% colnames(clean_data)) %>%
mutate(Action = "remove",
Rationale = "")
new_cols <- data.frame(Variable = colnames(clean_data)) %>%
filter(! x %in% colnames(raw_data)) %>%
mutate(Action = "added",
Rationale = "")
variable_tracker <- bind_rows(removed_cols, new_cols)
}
create_variable_tracker(raw_data = raw_data, clean_data = )
create_variable_tracker <- function(raw_data, clean_data) {
removed_cols <- data.frame(Variable = colnames(raw_data)) %>%
filter(! Variable %in% colnames(clean_data)) %>%
mutate(Action = "remove",
Rationale = "")
new_cols <- data.frame(Variable = colnames(clean_data)) %>%
filter(! Variable %in% colnames(raw_data)) %>%
mutate(Action = "added",
Rationale = "")
variable_tracker <- bind_rows(removed_cols, new_cols)
}
create_variable_tracker(raw_data = raw_data, clean_data = )
create_variable_tracker(raw_data = raw_data, clean_data = cleaned_data_weighted)
create_variable_tracker <- function(raw_data, clean_data) {
removed_cols <- data.frame(Variable = colnames(raw_data)) %>%
filter(! Variable %in% colnames(clean_data)) %>%
mutate(Action = "remove",
Rationale = "")
new_cols <- data.frame(Variable = colnames(clean_data)) %>%
filter(! Variable %in% colnames(raw_data)) %>%
mutate(Action = "added",
Rationale = "")
variable_tracker <- bind_rows(removed_cols, new_cols)
}
create_variable_tracker(raw_data = raw_data_no_pii, clean_data = main_data_no_pii)
cols_to_remove <- c("consent_no", "enumerator_sensitivity", "deviceid", "audit", "enumerator_ID",
"resp_phone", "resp_age", "_submission_time", "_validation_status",
"_notes", "_status", "_submitted_by", "__version__", "_tags", "_index", "audit_URL")
raw_data_no_pii <- raw_data %>%
select(-any_of(cols_to_remove))
main_data_no_pii <- main_data %>%
select(-any_of(cols_to_remove))
deletion <- deletion %>%
left_join(raw_data %>%  select(`_uuid`, enumerator_ID), by = join_by("uuid" == "_uuid"))
create_variable_tracker(raw_data = raw_data_no_pii, clean_data = main_data_no_pii)
return(variable_tracker)
create_variable_tracker <- function(raw_data, clean_data) {
removed_cols <- data.frame(Variable = colnames(raw_data)) %>%
filter(! Variable %in% colnames(clean_data)) %>%
mutate(Action = "remove",
Rationale = "")
new_cols <- data.frame(Variable = colnames(clean_data)) %>%
filter(! Variable %in% colnames(raw_data)) %>%
mutate(Action = "added",
Rationale = "")
variable_tracker <- bind_rows(removed_cols, new_cols)
return(variable_tracker)
}
create_variable_tracker(raw_data = raw_data_no_pii, clean_data = main_data_no_pii)
create_variable_tracker(raw_data = raw_data, clean_data = main_data_no_pii)
get_kobo_metadata <- function(dataset, asset_id, un, remove_geo = TRUE) {
# Try to retrieve password from keyring
pw <- tryCatch({
keyring::key_get(un)
}, error = function(e) {
keyring::key_set(un, prompt = "Please input your Kobo password. You will only have to do this once.")
keyring::key_get(un)
})
# Get token and setup Kobo API
kobo_server_url <- "https://kobo.impact-initiatives.org/"
kobo_token_value <- robotoolbox::kobo_token(
username = un,
password = pw,
url = sub("\\/$", "", kobo_server_url)
)
robotoolbox::kobo_setup(
url = sub("\\/$", "", kobo_server_url),
token = kobo_token_value
)
# Retrieve and process audit files
audit_files <- robotoolbox::kobo_audit(x = asset_id, progress = TRUE)
audit_files_length <- audit_files %>%
dplyr::mutate(metadata_duration = (end_int - start_int) / 60000)
if (remove_geo) {
audit_files_length <- audit_files_length %>%
dplyr::filter(!stringr::str_detect(node, "geo"))
}
iv_lengths <- audit_files_length %>%
dplyr::group_by(`_id`) %>%
dplyr::summarise(
interview_duration = sum(metadata_duration, na.rm = TRUE),
start_time_metadata = first(start),
.groups = "drop"
)
data_in_processing <- dataset %>%
dplyr::left_join(iv_lengths, by = "_id")
return(list(
df_and_duration = data_in_processing,
audit_files_length = audit_files_length
))
}
kobo_data_metadata <- get_kobo_metadata(dataset = my_raw_dataset, asset_id = "a78BPkkB3ZDwihdL5uqd5J", username = "abdirahmanaia")
kobo_data_metadata <- get_kobo_metadata(dataset = my_raw_dataset, asset_id = "a78BPkkB3ZDwihdL5uqd5J", un = "abdirahmanaia")
test_output <- kobo_data_metadata <- get_kobo_metadata(dataset = my_raw_dataset, asset_id = "a78BPkkB3ZDwihdL5uqd5J", un = "abdirahmanaia")
is.na(my_raw_dataset)
is.null(my_raw_dataset)
exists(my_raw_dataset)
exists("my_raw_dataset")
test <- data.frame(x = 2)
exists("test")
exists(test)
if(! exists("dataset")) {
stop("Dataset not found")
}
get_kobo_metadata <- function(dataset, asset_id, un, remove_geo = TRUE) {
if(! exists("dataset")) {
stop("Dataset not found")
}
# Try to retrieve password from keyring
pw <- tryCatch({
keyring::key_get(un)
}, error = function(e) {
keyring::key_set(un, prompt = "Please input your Kobo password. You will only have to do this once.")
keyring::key_get(un)
})
# Get token and setup Kobo API
kobo_server_url <- "https://kobo.impact-initiatives.org/"
kobo_token_value <- robotoolbox::kobo_token(
username = un,
password = pw,
url = sub("\\/$", "", kobo_server_url)
)
robotoolbox::kobo_setup(
url = sub("\\/$", "", kobo_server_url),
token = kobo_token_value
)
# Retrieve and process audit files
audit_files <- robotoolbox::kobo_audit(x = asset_id, progress = TRUE)
audit_files_length <- audit_files %>%
dplyr::mutate(metadata_duration = (end_int - start_int) / 60000)
if (remove_geo) {
audit_files_length <- audit_files_length %>%
dplyr::filter(!stringr::str_detect(node, "geo"))
}
iv_lengths <- audit_files_length %>%
dplyr::group_by(`_id`) %>%
dplyr::summarise(
interview_duration = sum(metadata_duration, na.rm = TRUE),
start_time_metadata = first(start),
.groups = "drop"
)
data_in_processing <- dataset %>%
dplyr::left_join(iv_lengths, by = "_id")
return(list(
df_and_duration = data_in_processing,
audit_files_length = audit_files_length
))
}
kobo_data_metadata <- get_kobo_metadata(dataset = my_raw_dataset, asset_id = "a78BPkkB3ZDwihdL5uqd5J")
kobo_data_metadata <- get_kobo_metadata(dataset = my_raw_dataset, asset_id = "a78BPkkB3ZDwihdL5uqd5J", un = "abdirahmanaia")
my_raw_dataset
exists("dataset")
get_kobo_metadata <- function(dataset, asset_id, un, remove_geo = TRUE) {
if (missing(dataset) || is.null(dataset) || !is.data.frame(dataset)) {
stop("Please provide a valid data frame to the `dataset` argument.")
}
# Try to retrieve password from keyring
pw <- tryCatch({
keyring::key_get(un)
}, error = function(e) {
keyring::key_set(un, prompt = "Please input your Kobo password. You will only have to do this once.")
keyring::key_get(un)
})
# Get token and setup Kobo API
kobo_server_url <- "https://kobo.impact-initiatives.org/"
kobo_token_value <- robotoolbox::kobo_token(
username = un,
password = pw,
url = sub("\\/$", "", kobo_server_url)
)
robotoolbox::kobo_setup(
url = sub("\\/$", "", kobo_server_url),
token = kobo_token_value
)
# Retrieve and process audit files
audit_files <- robotoolbox::kobo_audit(x = asset_id, progress = TRUE)
audit_files_length <- audit_files %>%
dplyr::mutate(metadata_duration = (end_int - start_int) / 60000)
if (remove_geo) {
audit_files_length <- audit_files_length %>%
dplyr::filter(!stringr::str_detect(node, "geo"))
}
iv_lengths <- audit_files_length %>%
dplyr::group_by(`_id`) %>%
dplyr::summarise(
interview_duration = sum(metadata_duration, na.rm = TRUE),
start_time_metadata = first(start),
.groups = "drop"
)
data_in_processing <- dataset %>%
dplyr::left_join(iv_lengths, by = "_id")
return(list(
df_and_duration = data_in_processing,
audit_files_length = audit_files_length
))
}
kobo_data_metadata <- get_kobo_metadata(dataset = my_raw_dataset, asset_id = "a78BPkkB3ZDwihdL5uqd5J", un = "abdirahmanaia")
if (missing(dataset) || !is.data.frame(dataset)) {
stop("Please provide a valid data frame to the `dataset` argument.")
}
get_kobo_metadata <- function(dataset, asset_id, un, remove_geo = TRUE) {
if (missing(dataset) || !is.data.frame(dataset)) {
stop("Please provide a valid data frame to the `dataset` argument.")
}
# Try to retrieve password from keyring
pw <- tryCatch({
keyring::key_get(un)
}, error = function(e) {
keyring::key_set(un, prompt = "Please input your Kobo password. You will only have to do this once.")
keyring::key_get(un)
})
# Get token and setup Kobo API
kobo_server_url <- "https://kobo.impact-initiatives.org/"
kobo_token_value <- robotoolbox::kobo_token(
username = un,
password = pw,
url = sub("\\/$", "", kobo_server_url)
)
robotoolbox::kobo_setup(
url = sub("\\/$", "", kobo_server_url),
token = kobo_token_value
)
# Retrieve and process audit files
audit_files <- robotoolbox::kobo_audit(x = asset_id, progress = TRUE)
audit_files_length <- audit_files %>%
dplyr::mutate(metadata_duration = (end_int - start_int) / 60000)
if (remove_geo) {
audit_files_length <- audit_files_length %>%
dplyr::filter(!stringr::str_detect(node, "geo"))
}
iv_lengths <- audit_files_length %>%
dplyr::group_by(`_id`) %>%
dplyr::summarise(
interview_duration = sum(metadata_duration, na.rm = TRUE),
start_time_metadata = first(start),
.groups = "drop"
)
data_in_processing <- dataset %>%
dplyr::left_join(iv_lengths, by = "_id")
return(list(
df_and_duration = data_in_processing,
audit_files_length = audit_files_length
))
}
if (missing(dataset) || !is.data.frame(dataset)) {
stop("Please provide a valid data frame to the `dataset` argument.")
}
utils::remove.packages("ImpactFunctions")
devtools::install_github("alex-stephenson/ImpactFunctions", ref = "testing")
ImpactFunctions::get_kobo_data
ImpactFunctions::get_kobo_metadata()
ImpactFunctions::get_kobo_metadata
ImpactFunctions::get_kobo_metadata
?>devtools::install_github()
?devtools::install_github()
utils::remove.packages("ImpactFunctions")
ImpactFunctions::get_kobo_metadata
utils::remove.packages("ImpactFunctions")
devtools::install_github("alex_stephenson/ImpactFunctions", ref = "testing")
devtools::install_github("alex-stephenson/ImpactFunctions", ref = "testing")
ImpactFunctions::get_kobo_metadata
packageDescription("ImpactFunctions")$Version
remotes::install_github("alex-stephenson/ImpactFunctions@testing", upgrade = "never", force = TRUE)
ImpactFunctions::get_kobo_metadata
utils::remove.packages("ImpactFunctions")
ImpactFunctions::get_kobo_data
remove.packages("ImpactFunctions")
ImpactFunctions::get_kobo_data
search()
detach("package:ImpactFunctions", unload = TRUE, force = TRUE)
remotes::install_github("alex-stephenson/ImpactFunctions@testing", force = TRUE)
ImpactFunctions::get_kobo_metadata
packageDescription("ImpactFunctions")$RemoteRef  # should say 'testing'
detach("package:ImpactFunctions", unload = TRUE, force = TRUE)
utils::remove.packages("ImpactFunctions")
remotes::install_github("alex-stephenson/ImpactFunctions@testing", force = TRUE)
ImpactFunctions::get_kobo_metadata
packageDescription("ImpactFunctions")$RemoteRef  # should say 'testing'
remotes::install_github("alex-stephenson/ImpactFunctions@testing")
remotes::install_github("alex-stephenson/ImpactFunctions@testing", force = TRUE)
library(ImpactFunctions)
library(ImpactFunctions)
ImpactFunctions::get_kobo_metadata
my_raw_dataset <- ImpactFunctions::get_kobo_data(asset_id = "a78BPkkB3ZDwihdL5uqd5J", un = "abdirahmanaia")
kobo_data_metadata <- get_kobo_metadata(dataset = my_raw_dataset, asset_id = "a78BPkkB3ZDwihdL5uqd5J")
kobo_data_metadata <- get_kobo_metadata(dataset = my_raw_dataset, un = "abdirahmanaia", asset_id = "a78BPkkB3ZDwihdL5uqd5J")
kobo_data_metadata
my_data_with_indicators
my_data_with_indicators <- my_clean_data_deleted %>%
add_fcs(
cutoffs = "alternative",
fsl_fcs_cereal = "FCSStapCer7",
fsl_fcs_legumes = "FCSPr7",
fsl_fcs_veg = "FCSVeg7",
fsl_fcs_fruit = "FCSFruit7",
fsl_fcs_meat = "FCSMeatFish7",
fsl_fcs_dairy = "FCSDairy7",
fsl_fcs_sugar = "FCSSugar7",
fsl_fcs_oil = "FCSFat7"
) %>%
add_rcsi(
fsl_rcsi_lessquality = "rCSILessQlty",
fsl_rcsi_borrow  = "rCSIBorrow",
fsl_rcsi_mealsize = "rCSIMealSize",
fsl_rcsi_mealadult = "rCSIMealAdult",
fsl_rcsi_mealnb = "rCSIMealNb"
) %>%
add_hhs(
fsl_hhs_nofoodhh = "HHSNoFood",
fsl_hhs_nofoodhh_freq = "HHSNoFood_FR",
fsl_hhs_sleephungry = "HHSBedHung",
fsl_hhs_sleephungry_freq = "HHSBedHung_FR",
fsl_hhs_alldaynight = "HHSNotEat",
fsl_hhs_alldaynight_freq = "HHSNotEat_FR",
yes_answer = "yes",
no_answer = "no",
rarely_answer = "rarely",
sometimes_answer = "sometimes",
often_answer = "often"
) %>%
add_lcsi(
fsl_lcsi_stress1 = "LhCSIDomAsset",
fsl_lcsi_stress2 = "LhCSICrdtFood",
fsl_lcsi_stress3 = "LhCSISavingUrban",
fsl_lcsi_stress4 = "LhCSIBorrowCash",
fsl_lcsi_crisis1 = "LhCSIProdAsset",
fsl_lcsi_crisis2 = "LhCSIHealthEdu",
fsl_lcsi_crisis3 = "LhCSIOutSchool",
fsl_lcsi_emergency1 = "LhCSIHouseLand",
fsl_lcsi_emergency2 = "LhCSIBegged",
fsl_lcsi_emergency3 = "LhCSIMoveUrban",
yes_val = "yes",
no_val = "no",
exhausted_val = "no_exhausted",
not_applicable_val = "n/a"
)
questions <- read_excel("inputs/IRF_ENDLINE_TOOL_FEB2025.xlsx", sheet = "survey")
library(cleaningtools)
library(tidyverse)
library(readxl)
library(analysistools)
library(addindicators)
library(impactR4PHU)
questions <- read_excel("inputs/IRF_ENDLINE_TOOL_FEB2025.xlsx", sheet = "survey")
questions <- read_excel("inputs/IRF_ENDLINE_TOOL_FEB2025.xlsx", sheet = "survey")
choices <- read_excel("inputs/IRF_ENDLINE_TOOL_FEB2025.xlsx", sheet = "choices")
questions %>% filter(name %in% ("protection_concern_taxation", "experience_problem", "problrm_accessing_money", "encounter_challenege", "if_no_raised_concern", "pressure_share", "made_decision", "hh_conflict", "jealousy", "concern_price_increase", "community_price", "if_suggest", "received_assis_type", "main_source_incomes", "expenditure_extra", "basic_needs_unmet", service_cash, "cafimad_plus", "result_jealousy", "if_yes_concern_price_increase", "community_price_goods", "suggestions", "ngo_assistance_yes", "Savings", "amount_shared", "savings_Have"))
questions %>% filter(name %in% c("protection_concern_taxation", "experience_problem", "problrm_accessing_money", "encounter_challenege", "if_no_raised_concern", "pressure_share", "made_decision", "hh_conflict", "jealousy", "concern_price_increase", "community_price", "if_suggest", "received_assis_type", "main_source_incomes", "expenditure_extra", "basic_needs_unmet", service_cash, "cafimad_plus", "result_jealousy", "if_yes_concern_price_increase", "community_price_goods", "suggestions", "ngo_assistance_yes", "Savings", "amount_shared", "savings_Have"))
questions %>% filter(name %in% c("protection_concern_taxation", "experience_problem", "problrm_accessing_money", "encounter_challenege", "if_no_raised_concern", "pressure_share", "made_decision", "hh_conflict", "jealousy", "concern_price_increase", "community_price", "if_suggest", "received_assis_type", "main_source_incomes", "expenditure_extra", "basic_needs_unmet", "service_cash", "cafimad_plus", "result_jealousy", "if_yes_concern_price_increase", "community_price_goods", "suggestions", "ngo_assistance_yes", "Savings", "amount_shared", "savings_Have"))
questions %>% filter(name %in% c("protection_concern_taxation", "experience_problem", "problrm_accessing_money", "encounter_challenege", "if_no_raised_concern", "pressure_share", "made_decision", "hh_conflict", "jealousy", "concern_price_increase", "community_price", "if_suggest", "received_assis_type", "main_source_incomes", "expenditure_extra", "basic_needs_unmet", "service_cash", "cafimad_plus", "result_jealousy", "if_yes_concern_price_increase", "community_price_goods", "suggestions", "ngo_assistance_yes", "Savings", "amount_shared", "savings_Have")) %>% View()
len(c("protection_concern_taxation", "experience_problem", "problrm_accessing_money", "encounter_challenege", "if_no_raised_concern", "pressure_share", "made_decision", "hh_conflict", "jealousy", "concern_price_increase", "community_price", "if_suggest", "received_assis_type", "main_source_incomes", "expenditure_extra", "basic_needs_unmet", "service_cash", "cafimad_plus", "result_jealousy", "if_yes_concern_price_increase", "community_price_goods", "suggestions", "ngo_assistance_yes", "Savings", "amount_shared", "savings_Have"))
issues <- c("protection_concern_taxation", "experience_problem", "problrm_accessing_money", "encounter_challenege", "if_no_raised_concern", "pressure_share", "made_decision", "hh_conflict", "jealousy", "concern_price_increase", "community_price", "if_suggest", "received_assis_type", "main_source_incomes", "expenditure_extra", "basic_needs_unmet", "service_cash", "cafimad_plus", "result_jealousy", "if_yes_concern_price_increase", "community_price_goods", "suggestions", "ngo_assistance_yes", "Savings", "amount_shared", "savings_Have")
questions %>% filter(name %in% c("protection_concern_taxation", "experience_problem", "problrm_accessing_money", "encounter_challenege", "if_no_raised_concern", "pressure_share", "made_decision", "hh_conflict", "jealousy", "concern_price_increase", "community_price", "if_suggest", "received_assis_type", "main_source_incomes", "expenditure_extra", "basic_needs_unmet", "service_cash", "cafimad_plus", "result_jealousy", "if_yes_concern_price_increase", "community_price_goods", "suggestions", "ngo_assistance_yes", "Savings", "amount_shared", "savings_Have")) %>% select(-3) %>% View()
raw_data <- read_excel("inputs/as_raw_data.xlsx") %>%
select(-savings)
questions %>% filter(name %in% c(name == "amount_shared")) %>% View()
questions %>% filter(name %in% c(name == "amount_shared"))
questions %>% filter(name == "amount_shared"))
questions %>% filter(name == "amount_shared")
main_data <- read_excel(file_path, 'cleaned_data')
# load datasets for processing
file_path <- "output/clean_and_raw.xlsx"
main_data <- read_excel(file_path, 'cleaned_data')
raw_data$amount_shared
## tool
kobo_tool_name <- "inputs/IRF_ENDLINE_TOOL_FEB2025.xlsx"
kobo_survey <- read_excel(kobo_tool_name, sheet = "survey") %>%
mutate(type = str_squish(type)) %>%
mutate(`label::English` = ifelse(
`label::English` == "If yes, which goods and commodity prices have increased?" & name == "community_price_goods", "If yes, which community goods and commodity prices have increased?", `label::English`))
kobo_survey %>% filter(name == "amount_shared"0)
kobo_survey %>% filter(name == "amount_shared")
questions %>% filter(name %in% c("protection_concern_taxation", "experience_problem", "problrm_accessing_money", "encounter_challenege", "if_no_raised_concern", "pressure_share", "made_decision", "hh_conflict", "jealousy", "concern_price_increase", "community_price", "if_suggest", "received_assis_type", "main_source_incomes", "expenditure_extra", "basic_needs_unmet", "service_cash", "cafimad_plus", "result_jealousy", "if_yes_concern_price_increase", "community_price_goods", "suggestions", "ngo_assistance_yes", "Savings", "amount_shared", "savings_Have")) %>% View()
